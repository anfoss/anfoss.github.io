---
title: 'More on neural networks for protease prediction'
date: 2020-09-21
permalink: /posts/2020/09/hiv_dnn2/
tags:
  - Deep learning
  - Proteomics
  - python3
  - HIV
---


In the last [post](https://anfoss.github.io/posts/2020/09/hiv_dnn/) we tried to predict HIV protease substrates. While training our first deep neural network we observed that it was not *learning* but rather *remembering* the training data aka overfitting.
Here we will try to fix it and introduce some more deep learning concepts.

First we will quickly regenerate the training and test data


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Input
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_auc_score


def qc_plots(history, pre='100units2hidden'):
    plt.subplot(211)
    plt.ylabel('Loss')
    plt.plot(history.history['loss'], label='train')
    plt.plot(history.history['val_loss'], label='val')
    plt.legend()
    plt.subplot(212)
    plt.ylabel('Accuracy')
    plt.plot(history.history['acc'], label='train')
    plt.plot(history.history['val_acc'], label='val')
    plt.xlabel('Training epoch')
    plt.legend()
    plt.savefig('{}_training.pdf'.format(pre), bbox_inches='tight', dpi=1600)
    plt.show()


def preprocess_seq(df, aa):
    """
    receive a two column df with df[0] is a peptide and df[1] is class label
    returns encoded features as integer and labels
    """
    char_dict = {}
    for index, val in enumerate(aa):
        char_dict[val] = index+1

    # now that we have a dictionary we can just split the sequence in multiple column
    sequences = df[0].str.split('', expand=True)

    # and replace every letter with a number
    for colname in list(sequences):
        sequences[colname] = sequences[colname].str.upper().map(char_dict)

    # there are empty columns, we are just going to drop them
    sequences.dropna(axis=1, inplace=True)
    return sequences.values, df[1].values


aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',
         'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']


data1 = pd.read_csv('746Data.txt', sep=",", header=None)
data2 = pd.read_csv('1625Data.txt', sep=",", header=None)
data3 = pd.read_csv('impensData.txt', sep=",", header=None)
test_data = pd.read_csv('schillingData.txt', sep=",", header=None)
tots = pd.concat([data1, data2, data3])
X, y = preprocess_seq(tots, aa)
X_test, y_test = preprocess_seq(test_data, aa)
y[y == -1] = 0
y_test[y_test == -1] = 0
X_train, X_val, y_train, y_val = train_test_split(X, y,
                                                    test_size=0.2,
                                                    random_state=42,
                                                    stratify=y)
```

Here is the last model that we used, with 2 hidden layers and 100 neurons each


```python
from numpy.random import seed
seed(1)

model = Sequential()
model.add(Dense(units = 100, input_dim=X.shape[1], activation = 'relu', name='hidden1'))
model.add(Dense(units = 100, activation = 'relu', name='hidden2'))
model.add(Dense(units = 1, activation = 'sigmoid', name='output'))
model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['acc'], lr=0.001)
model.summary()
```

    Model: "sequential_1"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    hidden1 (Dense)              (None, 100)               900       
    _________________________________________________________________
    hidden2 (Dense)              (None, 100)               10100     
    _________________________________________________________________
    output (Dense)               (None, 1)                 101       
    =================================================================
    Total params: 11,101
    Trainable params: 11,101
    Non-trainable params: 0
    _________________________________________________________________


While this model performed good on the training data it did not seem to learn anything on the validation data as the accuracy was stable for every epoch (cycle of training).
Several strategies for dealing with this situations are possible and we will try some of them.
First we will fit the original model and then try several different models with some tweaks to evaluate if we can increase performance.


```python
def metrics(model, X_test, y_test, nm):
    yhat_probs = model.predict(X_test, verbose=0)
    yhat_classes = model.predict_classes(X_test, verbose=0)
    yhat_probs = yhat_probs[:, 0]
    yhat_classes = yhat_classes[:, 0]

    accuracy = accuracy_score(y_test, yhat_classes)
    precision = precision_score(y_test, yhat_classes)
    recall = recall_score(y_test, yhat_classes)
    f1 = f1_score(y_test, yhat_classes)
    auc = roc_auc_score(y_test, yhat_probs)
    return [nm, accuracy, precision, recall, f1, auc]


history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=100,
                    batch_size=128,
                    verbose=0
                    )
base_perf = metrics(model, X_test, y_test, nm='base dnn')
```

### Save the best performing model (not the last one)

Instead of selecting the model *at the end* of the training, we can pick the model which *performed* better on the validation set as this set is evaluated at every epoch.

For this, we will save the weights of the best model and then reload them into a empty architecture (i.e not trained network).

This is pretty much a thing we will always do to keep the best performing model and reduce overfitting.


```python
from tensorflow.keras.models import load_model
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping

checkpoint_name = 'tmp_weights.hdf5'

# we will save the model achieving highest accuracy (val_acc) and mode max
# or mode with lowest loss (preferred)
checkpoint = ModelCheckpoint(checkpoint_name,
                             monitor='val_acc',
                             verbose = 0,
                             save_best_only = True,
                             mode ='max',
                             patience=10
                            )
callbacks_list = [checkpoint]
history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=100,
                    batch_size=128,
                    verbose=0,
                    callbacks=callbacks_list)

# load weight into current structure
model.load_weights(checkpoint_name)

best_epoch_perf = metrics(model, X_test, y_test, nm='best_epoch_dnn')
```

**Reduce model size!**

While random forest or ensemble algorithms benefits by using the largest possible number of classifiers, deep learning models are prone to overfit in large networks and is important to understand the relationship between the number of features, the number of neurons and the number of layers.

So our architecture (2 layers with 100 neurons) might just be *too powerful for our task!*
Let's try a easier one with 2 layers with 20 neurons


```python
model = Sequential()
model.add(Dense(units = 20, input_dim=X.shape[1], activation = 'relu', name='hidden1'))
model.add(Dense(units = 20,  activation = 'relu', name='hidden2'))
model.add(Dense(units = 1, activation = 'sigmoid', name='output'))
model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['acc'], lr=0.001)
model.summary()
```

    Model: "sequential_2"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    hidden1 (Dense)              (None, 20)                180       
    _________________________________________________________________
    hidden2 (Dense)              (None, 20)                420       
    _________________________________________________________________
    output (Dense)               (None, 1)                 21        
    =================================================================
    Total params: 621
    Trainable params: 621
    Non-trainable params: 0
    _________________________________________________________________



```python
def get_checkpoint(fl, metr):
    modes = {'val_acc':'max', 'val_loss':'min'}
    checkpoint = ModelCheckpoint(fl,
                                 monitor=metr,
                                 verbose = 0,
                                 save_best_only = True,
                                 mode =modes[metr],
                                 patience=10
                                )
    return [checkpoint]


history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=100,
                    batch_size=128,
                    verbose=0,
                    callbacks=get_checkpoint('tmp_weights.hdf5', 'val_loss'))

model.load_weights(checkpoint_name)
simple_dnn_perf = metrics(model, X_test, y_test, nm='simple dnn')
qc_plots(history, 'simple dnn')
```


![](/images/simple dnn_training.pdf)


**Wow, what a difference!** we can see clearly that now our model is learning something but poorly (loss for the training set is kinda high). But compared to before the validation accuracy and loss follows the same trend as the training one which is *exactly what should happen*.
Let's stick with this architecture and how we can further improve it

### Batch size and learning


Batch size as we seen before is a even-size chunk of data. For example we used 128 which means the model will go through the entire dataset 128 peptides at the time.
As intuition we can think about batch size this way:

- *Smaller batches such as 32 or 64 are noisy (as they resemble less the original training data) and can help with overfitting* however it is possible that the batches will be too different resulting in not learning anything
- Large batches represents better the training data and help the model converge to a *global optimum* rather than a local optimum but they are slower to train


```python
model.summary()
history1024batch = model.fit(X_train, y_train,
                           validation_data=(X_val, y_val),
                           epochs=100,
                           batch_size=1024,
                           verbose=0,
                           callbacks=get_checkpoint('tmp_weights.hdf5', 'val_loss')
                           )

batch1024_perf = metrics(model, X_test, y_test, nm='1024 batch size dnn')


history32batch = model.fit(X_train, y_train,
                           validation_data=(X_val, y_val),
                           epochs=100,
                           batch_size=32,
                           verbose=0,
                           callbacks=get_checkpoint('tmp_weights.hdf5', 'val_loss')
                           )

batch32_perf = metrics(model, X_test, y_test, nm='32 batch size dnn')
qc_plots(history1024batch, '1024 batch size')
```

    Model: "sequential_3"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    hidden1 (Dense)              (None, 20)                180       
    _________________________________________________________________
    dropout (Dropout)            (None, 20)                0         
    _________________________________________________________________
    hidden2 (Dense)              (None, 20)                420       
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 20)                0         
    _________________________________________________________________
    output (Dense)               (None, 1)                 21        
    =================================================================
    Total params: 621
    Trainable params: 621
    Non-trainable params: 0
    _________________________________________________________________





### Dropout to increase generalization

A popular way to increase the model capability to generalize is to use [Dropout](https://en.wikipedia.org/wiki/Dropout_(neural_networks)). The idea is that we can remove randomly some connection at every epoch (i.e *making the model forget things*) and this forces the model to not rely on specific connections but rather to learn a general representation of our data.


```python
from tensorflow.keras.layers import Dropout

model = Sequential()
model.add(Dense(units = 20, input_dim=X.shape[1], activation = 'relu', name='hidden1'))
# drop 20% of connections
model.add(Dropout(0.2))
model.add(Dense(units = 20, activation = 'relu', name='hidden2'))
model.add(Dropout(0.2))
model.add(Dense(units = 1, activation = 'sigmoid', name='output'))
model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['acc'], lr=0.001)
```


```python
historydropout = model.fit(X_train, y_train,
                           validation_data=(X_val, y_val),
                           epochs=100,
                           batch_size=64,
                           verbose=0,
                           callbacks=get_checkpoint('tmp_weights.hdf5', 'val_loss')
                           )
dropout_perf = metrics(model, X_test, y_test, nm='dropout dnn')
```


### Final trick (for now)

We tried tweaking the model with very mild improvements. So what to do now?
*We need to think about what we are trying to classify*

It boils down to **which features are we using to classify what**.

We are dealing with a peptide which means the *order* is important aka **aminoacid X needs to be in position Y** to be a cleavable peptide. Now we are NOT loooking at the order but mostly on the composition.

I will go into types of neural network architecture dealing with sequential data in a follow up post where I will explain how this mysterious dnn achieved such a high performance (and how these nets can be used to perform very interesting tasks in proteomics such as peptide fragmentation prediction)


```python
mist_perf = metrics(lstmbi, X_test, y_test, nm='mysterious dnn')
```

Finally to compare with all models we trained before, we will also quickly re-train the random forest model we used before and compare its performance with all the neural nets we used so far


```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix

clf_rf = RandomForestClassifier(random_state=42, n_estimators=1000)
clf_rf=clf_rf.fit(X_train,y_train)
y_pred = clf_rf.predict(X_test)
tn,fp,fn,tp = confusion_matrix(y_test, y_pred).ravel()

recall = tp / (tp + fn)
precision = tp / (tp + fp)
accuracy = (tp+tn) / (tp+fp+tn+fn)
f1 = 2 * ((precision*recall) / (precision+recall))
auc = roc_auc_score(y_test, clf_rf.predict_proba(X_test)[:,1])

rf_metr = ['rf', accuracy, precision, recall, f1, auc]
```


```python
ls = [dropout_perf, base_perf, batch32_perf, batch1024_perf, mist_perf, rf_metr]
perf_df = pd.DataFrame(ls, columns=['clf', 'accuracy', 'precision', 'recall', 'f1', 'auc'])
perf_df
```


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>clf</th>
      <th>accuracy</th>
      <th>precision</th>
      <th>recall</th>
      <th>f1</th>
      <th>auc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>dropout dnn</td>
      <td>0.856357</td>
      <td>0.153846</td>
      <td>0.018433</td>
      <td>0.032922</td>
      <td>0.514406</td>
    </tr>
    <tr>
      <th>1</th>
      <td>base dnn</td>
      <td>0.800733</td>
      <td>0.175595</td>
      <td>0.135945</td>
      <td>0.153247</td>
      <td>0.570248</td>
    </tr>
    <tr>
      <th>2</th>
      <td>32 batch size dnn</td>
      <td>0.854829</td>
      <td>0.098039</td>
      <td>0.011521</td>
      <td>0.020619</td>
      <td>0.515348</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1024 batch size dnn</td>
      <td>0.854218</td>
      <td>0.158730</td>
      <td>0.023041</td>
      <td>0.040241</td>
      <td>0.507113</td>
    </tr>
    <tr>
      <th>4</th>
      <td>mysterious dnn</td>
      <td>0.866443</td>
      <td>0.496350</td>
      <td>0.470046</td>
      <td>0.482840</td>
      <td>0.850084</td>
    </tr>
    <tr>
      <th>5</th>
      <td>rf</td>
      <td>0.870416</td>
      <td>0.569444</td>
      <td>0.094470</td>
      <td>0.162055</td>
      <td>0.831019</td>
    </tr>
  </tbody>
</table>
</div>




```python
def radarplot(df, nm):
    import math as m
    font = {'size': 9}
    plt.rc('font', **font)
    plt.rcParams["font.family"] = "arial"
    categories = list(df)[1:]
    N = len(categories)
    values = df.loc[0].drop('clf').values.flatten().tolist()
    values += values[:1]
    angles = [n / float(N) * 2 * m.pi for n in range(N)]
    angles += angles[:1]
    ax = plt.subplot(111, polar=True)
    ax.set_facecolor('#F0F0F0')

    plt.xticks(angles[:-1], categories, color='black', size=9)

    ax.set_rlabel_position(30)
    # min and max [0.72, 1], middle circle is 0.86
    #plt.yticks([0.72, 0.86, 1], ["0.72", "0.86", "1.0"], color="black", size=9)
    # plt.ylim(0.72,1)

    for idx, nm in enumerate(nm):
        values = df.loc[idx].drop('clf').values.flatten().tolist()
        values += values[:1]
        ax.plot(angles, values, linewidth=2, linestyle='solid', label=nm)
        # ax.fill(angles, values, 'b', alpha=0.1)


    # Add legend
    plt.legend(loc='right', bbox_to_anchor=(0, 0.25), fontsize=9)


    # Fill area
    ax.fill(angles, values, 'b', alpha=0.1)
    plt.savefig('./legend.pdf', dpi=600, bbox_inches='tight')
    plt.show()
    plt.close()

radarplot(perf_df, nm=list(perf_df['clf']))

```

![](/images/hivdlradarplot2.pdf)


### When you have a hammer everything should (not) look like a nail

We just spent a lot of time trying to optimize our feed-forward neural network to outperfom random forest and we did not see a huge improvement even from the base model, until we switched to a complicated architecture (*recurrent neural network*).

So the main takeway from this post is **do not use neural networks all the times only because they are easy to implement!**

In the following posts we will finish this series by looking at some (complicated) neural networks architecture using computer vision and recurrent neural networks and how they can be applied for some more advanced tasks such as peptide fragmentation prediction or retention time prediction.
