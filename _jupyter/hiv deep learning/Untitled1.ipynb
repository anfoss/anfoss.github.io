{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning demystified: HIV protease substrate prediction #2\n",
    "\n",
    "In the last post we tried to predict peptides cleaved by the HIV protease or not. While training our first deep neural network we observed that it was not *learning* but rather *remembering* the training data.\n",
    "Here we will try to fix it and introduce some more deep learning concepts.\n",
    "\n",
    "First we will quickly regenerate the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# let's read the files we dumped before\n",
    "\n",
    "X_train = np.loadtxt(\"X_train.csv\", delimiter=\",\")\n",
    "X_val = np.loadtxt(\"X_val.csv\",  delimiter=\",\")\n",
    "y_train = np.loadtxt(\"y_train.csv\" , delimiter=\",\")\n",
    "y_val = np.loadtxt(\"y_val.csv\", delimiter=\",\")\n",
    "X_test = np.loadtxt(\"X_test.csv\" , delimiter=\",\")\n",
    "y_test = np.loadtxt(\"y_test.csv\" , delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model that we used was this one were we used 2 hidden layers with 100 units each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden1 (Dense)              (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 11,101\n",
      "Trainable params: 11,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 100, input_dim=X.shape[1], activation = 'relu', name='hidden1'))\n",
    "model.add(Dense(units = 100, activation = 'relu', name='hidden2'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid', name='output'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['acc'], lr=0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this model performed good on the training data it did not seem to learn anything on the test data as the accuracy was stable for every epoch (cycle of training).\n",
    "Several strategies for dealing with this situations are possible and we will try some of them.\n",
    "First we will fit the original model and then create as before a radarplot to illustrate the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['base dnn', 0.9019607843137255, 0.9072847682119205, 0.7287234042553191, 0.8082595870206489, 0.9239081746920493]\n"
     ]
    }
   ],
   "source": [
    "def metrics(model, X_test, y_test, nm):\n",
    "    _, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    yhat_probs = model.predict(X_test, verbose=0)\n",
    "    yhat_classes = model.predict_classes(X_test, verbose=0)\n",
    "    yhat_probs = yhat_probs[:, 0]\n",
    "    yhat_classes = yhat_classes[:, 0]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, yhat_classes)\n",
    "    precision = precision_score(y_test, yhat_classes)\n",
    "    recall = recall_score(y_test, yhat_classes)\n",
    "    f1 = f1_score(y_test, yhat_classes)\n",
    "    auc = roc_auc_score(y_test, yhat_probs)\n",
    "    return [nm, accuracy, precision, recall, f1, auc] \n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=100, \n",
    "                    batch_size=256, \n",
    "                    # set verbose = 1 to have full output\n",
    "                    verbose=0\n",
    "                    )\n",
    "base_perf = metrics(model, X_test, y_test, nm='base dnn')\n",
    "print(base_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Reduce model size!\n",
    "\n",
    "While random forest or ensamble algorithms benefits by using the largest possible number of classifier neural networks are very prone to overfit in large networks and is important to understand the relationship between the number of features, the number of neurons and the number of layers.\n",
    "\n",
    "\n",
    "So our architecture (2 layers with 100 neurons) might just be *too powerful for our task!*\n",
    "Let's try a easier one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden1 (Dense)              (None, 50)                450       \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 3,051\n",
      "Trainable params: 3,051\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units = 50, input_dim=X.shape[1], activation = 'relu', name='hidden1'))\n",
    "model.add(Dense(units = 50, input_dim=X.shape[1], activation = 'relu', name='hidden2'))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid', name='output'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['acc'], lr=0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dropout dnn',\n",
       " 0.8883861236802413,\n",
       " 0.8131868131868132,\n",
       " 0.7872340425531915,\n",
       " 0.7999999999999999,\n",
       " 0.9294064949608061]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history8neuron = model.fit(X_train, y_train, \n",
    "                           validation_data=(X_val, y_val), \n",
    "                           epochs=100, \n",
    "                           batch_size=32, \n",
    "                           verbose=0\n",
    "                           )\n",
    "simple_dnn_perf = metrics(model, X_test, y_test, nm='dropout dnn')\n",
    "simple_dnn_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size and learning\n",
    "\n",
    "\n",
    "Batch size as we seen before is a even size chunk of data. For example we used 128 which means the model will go through the entire dataset 128 peptides at the time. \n",
    "\n",
    "- *Smaller batches are noisy (as they resemble less the original training data) and can help with overfitting* \n",
    "- Large batches represents better the training data and help the model converge to a *global optimum* rather than a local optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history32batch = model.fit(X_train, y_train, \n",
    "                           validation_data=(X_val, y_val), \n",
    "                           epochs=100, \n",
    "                           batch_size=32, \n",
    "                           verbose=0\n",
    "                           )\n",
    "\n",
    "batch32_perf = metrics(model, X_test, y_test, nm='32 batch size dnn')\n",
    "\n",
    "\n",
    "history32batch = model.fit(X_train, y_train, \n",
    "                           validation_data=(X_val, y_val), \n",
    "                           epochs=100, \n",
    "                           batch_size=64, \n",
    "                           verbose=0\n",
    "                           )\n",
    "\n",
    "batch64_perf = metrics(model, X_test, y_test, nm='64 batch size dnn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout to increase generalization\n",
    "\n",
    "A popular way to increase the model capability to generalize is to use Dropout. The idea is that we can remove randomly some connection at every epoch (i.e *making the model forget things*), thereby forcing the model to not rely on specific connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "hidden1 (Dense)              (None, 100)               900       \n",
      "_________________________________________________________________\n",
      "hidden2 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 11,101\n",
      "Trainable params: 11,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units = 100, input_dim=X.shape[1], activation = 'tanh', name='hidden1'))\n",
    "# drop 20% of connections\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 100, activation = 'relu', name='hidden2'))\n",
    "# drop 30 % of connection\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1, activation = 'sigmoid', name='output'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy',metrics=['acc'], lr=0.001)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dropout dnn',\n",
       " 0.7646699266503667,\n",
       " 0.1744186046511628,\n",
       " 0.2073732718894009,\n",
       " 0.18947368421052632,\n",
       " 0.5461397816986714]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "historydropout = model.fit(X_train, y_train, \n",
    "                           validation_data=(X_val, y_val), \n",
    "                           epochs=100, \n",
    "                           batch_size=32, \n",
    "                           verbose=0\n",
    "                           )\n",
    "dropout_perf = metrics(model, X_test, y_test, nm='dropout dnn')\n",
    "dropout_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
