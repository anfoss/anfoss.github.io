{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning for proteomics: an easy introduction\n",
    "In a lot of proteomics publications recently, machine learning algorithms are used to perform a variety of tasks such as sample classification, image segmentation or prediction of important features in a set of samples.\n",
    "\n",
    "In this series I want to explore a bit how to employ machine learning in omics/proteomics and in general some good do's and don't in machine learning applications, plus providing some Python3 code to exemplify some of the ideas.\n",
    "Only prerequisite is basic understanding of Python. I will drop explanation of things which I reckon be important but feel free to reach out for curiosities or similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case study using random forest to predict COVID19 severity\n",
    "\n",
    "Random forest one of the most basic learning algorithm around the block and the easiest to apply. For a detailed explanation, there are several resources such as [Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "One of the major application where random forest is employed in proteomics paper is scoring of proteins across samples and sample classification.\n",
    "Random forests can be conceptualized as a hierarchical stack of decision trees. \n",
    "\n",
    "![](/images/rf_clf.png)\n",
    "\n",
    "A decision tree boils down to a series of questions which allows to uniquely separate a group of samples from others.\n",
    "So to calculate which features (i.e petal length or width) contributes more to classification, we can just observe how many misclassified samples are left after every decision.\n",
    "\n",
    "This concept is known as Gini Impurity and is related to how pure a leaf is (how many samples of only one class are present) compared to the remaining ones.\n",
    "Features leading to purer leafs are more important in the overall classification compared to other.\n",
    "\n",
    " __So we can retrieve how important a feature is and this will tell us about important proteins/analytes in our sample__\n",
    "\n",
    "For this example I will use the COVID data from a recent Cell paper where a random forest classifier was used to classify severe and non-severe COVID19 patients based on metabolites and proteins.\n",
    "All data is available (here)[https://www.cell.com/cell/fulltext/S0092-8674(20)30627-9#supplementaryMaterial]\n",
    "\n",
    "So let's start coding!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages and prepare training and test datasets\n",
    "\n",
    "We import the data and prepare all data. Luckily for us, in the publication, the data is already separated in test and training set.\n",
    "Every ML model needs to be trained on a set of data and then the generalization (i.e how good the model learned our data) capabilities are tested on a independent datasets.\n",
    "\n",
    "If test data is not available usually the training data is split in a 0.75/0.25 training/test or 0.66/0.33 if there are sufficient data points\n",
    "Anyway, let's continue with out example. In the publication the data is already merged, we only need to get positive (severe covid-19) and negative (non-severe covid-19) assign it to a label.\n",
    "For this, we will use supplementary table 1 which has the patient informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1639, 33) (1590, 12)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_validate, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "train = pd.read_excel('mmc3.xlsx', sheet_name='Prot_and_meta_matrix')\n",
    "test = pd.read_excel('mmc4.xlsx', sheet_name='Prot_and_meta_matrix')\n",
    "print(train.shape, test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen there is a different number of features in training (1639) and test (1590). While for some algorithms this doesn't matter, random forest needs same number of features. So we will quickly fix it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[train['Proteins/Metabolites'].isin(test['Proteins/Metabolites'])]\n",
    "test = test[test['Proteins/Metabolites'].isin(train['Proteins/Metabolites'])]\n",
    "train.shape[0] == test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_features_split(info, df, sheet_name):\n",
    "    \"\"\"\n",
    "    utility function to preprocess data and retrieve target (labels) and features (numerical values)\n",
    "    \"\"\"\n",
    "    df.fillna(0, inplace=True)\n",
    "    ids = df[['Proteins/Metabolites', 'Gene Symbol']]\n",
    "    df.drop(['Proteins/Metabolites', 'Gene Symbol'], axis=1, inplace=True)\n",
    "    df = df.rename(columns=df.iloc[0]).drop(df.index[0])\n",
    "    # needs to have features as columns and samples as row (i.e wide format)\n",
    "    df = df.T\n",
    "    df['class'] = df.index.map(info)\n",
    "    return df.drop('class', axis=1).values, df['class'].values\n",
    "\n",
    "\n",
    "# we need to have positive and negative information (i.e severe and )\n",
    "info = pd.read_excel('mmc1.xlsx', index_col=0, sheet_name='Clinical_information')\n",
    "info = info[info['Group d'].isin([2,3])]\n",
    "info['class'] = [0 if x==2 else 1 for x in list(info['Group d'])]\n",
    "info = dict(zip(info.index, info['class']))\n",
    "\n",
    "# get training and test data in format for ml\n",
    "Xtrain,ytrain = target_features_split(info, train, sheet_name='Prot_and_meta_matrix')\n",
    "Xtest, ytest = target_features_split(info, test, sheet_name='Prot_and_meta_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base machine learning\n",
    "\n",
    "We can start by fitting a very simple model with all default parameters. For this, we initialized an empty classifier and then fit the data.\n",
    "It is very important in machine learning to always use a seed (random_state in sklearn) to ensure reproducibility of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy\n",
    "    \"\"\"\n",
    "    predictions = model.predict(test_features)\n",
    "    ov = np.equal(test_labels, predictions)\n",
    "    tp = ov[np.where(ov ==True)].shape[0]\n",
    "    return tp/predictions.shape[0]\n",
    "\n",
    "# now we train on the features from the training set\n",
    "clf_rf_default = RandomForestClassifier(random_state=42)\n",
    "clf_rf_default.fit(Xtrain, ytrain)\n",
    "print(evaluate(clf_rf_default, Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see we got 70% recall and made two mistakes in classification where we predicted non severe covid (i.e 0) instead of severe covid (1). \n",
    "\n",
    "Let's see if we can improve this by doing some parameter optimization. For this we will use a grid search. So we will generate a set of various parameters and test random combinations to train our model.\n",
    "Alternatively GridSearchCV can be used, where instead of using random combinations, all possible ones are tested.\n",
    "\n",
    "RandomizedSearchCV and GridSearchCV use what is known as __cross validation__ or CV, which is a machine learning technique for model training where the data is splitted into equal parts (fold) and then all but one folds are used to train the model and the last one to predict. In this way a more robust estimation of model performance can obtained, but __the final model should be train on all available data which usually yields the best performance__.\n",
    "For a more in depth explanation of cross validation, (here)[https://scikit-learn.org/stable/modules/cross_validation.html] there is an excellent introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 11.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RF_covid.clf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def random_search():\n",
    "    # Number of trees in random forest\n",
    "    n_estimators = [250, 1000, 5000]\n",
    "    \n",
    "    # Maximum number of levels in tree\n",
    "    max_depth = [int(x) for x in np.linspace(20, 400, num = 40)]\n",
    "\n",
    "    # Number of features to consider at every split\n",
    "    max_features = ['sqrt', 'log2']\n",
    "    # % samples required to split a node\n",
    "    min_samples_split = [2, 4, 6, 8, 10]\n",
    "    # Minimum % samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4, 8]\n",
    "    # Method of selecting samples for training each tree\n",
    "    bootstrap = [True, False]\n",
    "    # Create the random grid\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                   'max_features': max_features,\n",
    "                   'max_depth': max_depth,\n",
    "                   'min_samples_split': min_samples_split,\n",
    "                   'min_samples_leaf': min_samples_leaf,\n",
    "                   'bootstrap': bootstrap}\n",
    "    return random_grid\n",
    "\n",
    "# initialize parameter space\n",
    "grid = random_search()\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "\n",
    "# try 5 fold CV on 100 combinations aka 500 fits\n",
    "rf_opt = RandomizedSearchCV(estimator = clf,\n",
    "                       param_distributions = grid,\n",
    "                       cv = 5,\n",
    "                       n_iter=100,\n",
    "                       verbose=1,\n",
    "                       n_jobs = -1,\n",
    "                       scoring='roc_auc')\n",
    "\n",
    "# Fit the random search model\n",
    "rf_opt_grid=rf_opt.fit(Xtrain, ytrain)\n",
    "\n",
    "# retrieve best performing model\n",
    "clf_rf_opt = rf_opt.best_estimator_\n",
    "evaluate(clf_rf_opt, Xtest, ytest)\n",
    "'Performance increase by {}%'.format(evaluate(clf_rf_opt, Xtest, ytest) - evaluate(clf_rf_default, Xtest, ytest))\n",
    "\n",
    "# save classifier to a file\n",
    "joblib.dump(clf_rf_opt, 'RF_covid.clf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can manually inspect the performance at every combination of parameters directly and compare it with the reported one from the paper (AUC 0.957) as we used the same score ('roc_auc') in GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame.from_dict(rf_opt_grid.cv_results_)\n",
    "res['mean_test_score'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the performance increased by just tuning the model.\n",
    "We also achieved almost the same AUC (0.955) vs 0.957 reported in the paper so we manage to reproduce the classification. We could go further and double check which patient wasn't classified correctly, but let's take a break and continue in the next post!\n",
    "Now, in the next part we will look into feature importance to figure out which proteins and metabolites allowed us to separate the samples.\n",
    "We will also go in depth into some more tune-up and the problem of overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
